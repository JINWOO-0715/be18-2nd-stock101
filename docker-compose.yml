version: '3.8'

services:
  # Ollama - Local LLM Server
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    # Uncomment the following lines to enable GPU support (requires NVIDIA Container Toolkit)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]
    restart: always

  # Qdrant - Vector Database
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    restart: always

  # Optional: Ollama Web UI (Open WebUI) for easier interaction/testing
  # open-webui:
  #   image: ghcr.io/open-webui/open-webui:main
  #   container_name: open-webui
  #   ports:
  #     - "3000:8080"
  #   environment:
  #     - OLLAMA_BASE_URL=http://ollama:11434
  #   volumes:
  #     - open_webui_data:/app/backend/data
  #   depends_on:
  #     - ollama
  #   restart: always

volumes:
  ollama_data:
  qdrant_data:
  # open_webui_data:
